{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcHkQeZpji90"
   },
   "source": [
    "**Before Starting**: First, fill out the below code cell with your first name, last name, and student ID.\n",
    "\n",
    "**Before Submission**: Make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "\n",
    "**During Lab Tips**:\n",
    "1. DO NOT write your written responses in the same markdown cell as the question. If you do this, your written response will be lost!\n",
    "\n",
    "\n",
    "2. If possible, please try to use your local Jupyter Notebook to complete the lab. Online notebook editors like Collab can edit notebook source code and cause our auto-grader to break, making grading your lab more difficult for us!\n",
    "\n",
    "**<font color='red'>WARNING: Some TODOs have `todo_check()` functions which will give you a rough estimate of whether you will recieve points or not. <u>These checks are there simply to make sure you are on the right track and they DO NOT determine your final grade for the lab</u>. They are only here to provide you with real-time feedback.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCnRsi-Sji93"
   },
   "outputs": [],
   "source": [
    "FIRST_NAME = \"Claude\"\n",
    "LAST_NAME = \"Kouakou\"\n",
    "STUDENT_ID = \"801438848\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFlb6gWsji93"
   },
   "source": [
    "\n",
    "\n",
    "# Linear Regression Lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CufUq44eji94"
   },
   "outputs": [],
   "source": [
    "# Extra imports for this lab that are beyond the scope of discussion\n",
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import warnings\n",
    "from pdb import set_trace\n",
    "\n",
    "# Set this to True if you DO NOT want to run the\n",
    "# garbage_collect() functions throughout the notebook\n",
    "turn_off_garbage_collect = False\n",
    "\n",
    "def garbage_collect(vars_):\n",
    "    if not turn_off_garbage_collect:\n",
    "        for v in vars_:\n",
    "            if v in globals():\n",
    "                del globals()[v]\n",
    "        collected = gc.collect()\n",
    "\n",
    "\n",
    "class TodoCheckFailed(Exception):\n",
    "    pass\n",
    "\n",
    "def todo_check(asserts):\n",
    "    failed_err = \"You passed {}/{} and FAILED the following code checks:\\n{}\"\n",
    "    failed = \"\"\n",
    "    n_failed = 0\n",
    "    for check, (condi, err) in enumerate(asserts):\n",
    "        exc_failed = False\n",
    "        if isinstance(condi, str):\n",
    "            try:\n",
    "                passed = eval(condi)\n",
    "            except Exception:\n",
    "                exc_failed = True\n",
    "                n_failed += 1\n",
    "                failed += f\"\\nCheck [{check+1}]: Failed to execute check [{check+1}] due to the following error...\\n{traceback.format_exc()}\"\n",
    "        elif isinstance(condi, bool):\n",
    "            passed = condi\n",
    "        else:\n",
    "            raise ValueError(\"asserts must be a list of strings or bools\")\n",
    "\n",
    "        if not exc_failed and not passed:\n",
    "            n_failed += 1\n",
    "            failed += f\"\\nCheck [{check+1}]: Failed\\n\\tTip: {err}\\n\"\n",
    "\n",
    "    if len(failed) != 0:\n",
    "        passed = len(asserts) - n_failed\n",
    "        err = failed_err.format(passed, len(asserts), failed)\n",
    "        raise TodoCheckFailed(err.format(failed))\n",
    "    print(\"Your code PASSED the code check!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkgHS-Tiji94"
   },
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this activity is to introduce the popular ML tool, Scikit.Learn and practice linear regression models with it. You will apply ordinary least squares (LS), regularized linear regression models (i.e., Ridge, Lasso, and Elastic net), and online regression model (stochastic gradient descent) to real data. We will prepare data as we did in last week's practice and then apply these linear models. Follow the TODO titles and comments to finish the activity!\n",
    "\n",
    "# Agenda\n",
    "\n",
    "* Scikit.Learn Basics\n",
    "* Data Preparation\n",
    "  * Data Preprocessing\n",
    "  * Data Visualization\n",
    "  * Data Partitioning\n",
    "* Regression with  \n",
    "  * Least Squares\n",
    "  * Ridge Regression\n",
    "  * Lasso Regression\n",
    "  * Elastic Net\n",
    "  * Stochastic Gradient Descent\n",
    "  \n",
    "\n",
    "# Tables of TODO's\n",
    "\n",
    "\n",
    "1. [TODO1 (5 points)](#TODO1)\n",
    "2. [TODO2 (5 points)](#TODO2)\n",
    "3. [TODO3 (5 points)](#TODO3)\n",
    "4. [TODO4 (10 points)](#TODO4)  \n",
    "5. [TODO5 (5 points)](#TODO5)\n",
    "6. [TODO6 (5 points)](#TODO6)\n",
    "7. [TODO7 (8 points)](#TODO7)\n",
    "8. [TODO8 (10 points)](#TODO8)\n",
    "9. [TODO9 (5 points)](#TODO9)\n",
    "10. [TODO10 (5 points)](#TODO10)\n",
    "11. [TODO11 (20 points)](#TODO11)\n",
    "12. [TODO12 (5 points)](#TODO12)\n",
    "13. [TODO13 (10 points)](#TODO13)\n",
    "14. [Feedback (2 points)](#feedback)\n",
    "\n",
    "\n",
    "* Total: 100 Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DG56ys63ji94"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVn9eBqWji94"
   },
   "source": [
    "# Scikit.Learn\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/index.html) is one of the most popular machine learning tools. It is well managed by a lot of contributors and a good organizer group. The design of softeware is well developed, so it is easy to learn and apply to many different data anaylsis applications.\n",
    "\n",
    "By running the code cell below, if you have an error to import sklearn or the version is outdated, please follow [the instruction](https://scikit-learn.org/stable/install.html) to install/upgrade scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4cg37Dtji94",
    "outputId": "a5b8681a-7414-4ccc-f6a1-5719b2bfed7e"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_mMlX71ji95"
   },
   "source": [
    "Using scikit-learn is very simple. You can just follow five steps to use it in general:  <br/>\n",
    "&nbsp;&nbsp;1) import, <br/>\n",
    "&nbsp;&nbsp;2) prepare data, <br/>\n",
    "&nbsp;&nbsp;3) initialize (create an object),  <br/>\n",
    "&nbsp;&nbsp;4) train (fit), <br/>\n",
    "&nbsp;&nbsp;5) predict (or/and evaluate).  <br/>\n",
    "  \n",
    "Although you have already seen examples in the slides, we can give another one here. For now, you don't know need to know what KNN is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pP_BEnUji95",
    "outputId": "508d59c8-f949-4932-ff9f-49f4273135b4"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, neighbors           # 1) import\n",
    "\n",
    "X, t = datasets.load_digits(return_X_y=True)      # 2) prepare data (from library embeded one)\n",
    "X = X / X.max()                                   #.     - rescale the X to betweeen 0 and 1\n",
    "\n",
    "# number of data samples\n",
    "N = X.shape[0]\n",
    "\n",
    "# prepare train and test data\n",
    "i_split = int(0.8 * N)\n",
    "X_train = X[:i_split]\n",
    "t_train = t[:i_split]\n",
    "X_test = X[i_split:]\n",
    "t_test = t[i_split:]\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()            # 3) initialize - create an KNN classifier using default setting\n",
    "\n",
    "knn.fit(X_train, t_train)                         # 4) train with training data (input, X_train and target, t_train)\n",
    "\n",
    "knn.score(X_test, t_test)                         # 5) evaluate the model on the entire test data and compute the accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq8UQ5FUji96"
   },
   "source": [
    "The test accuracy seems good with 96 percent of accuracy. Well, we do not even know what the data is for, what the task is, and what ML model we used here. Let us see by actually see the data and predictions the trained model made to learn about them.\n",
    "\n",
    "Here, the data is famous hand-written digit recognition data. Therefore, the task is classify the image (8x8, 64 values after flattened) for right number (from 0 to 9). Here are the example codes to predict and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWoXq1Kgji96",
    "outputId": "517c2675-9de0-4486-90bd-ef147f1d69e1"
   },
   "outputs": [],
   "source": [
    "k = 8  # the number of data samples to check\n",
    "\n",
    "y = knn.predict(X_test[0:k])                     # 5) predict - for the first k data samples in X_test\n",
    "\n",
    "# plot the digits along with \"label (prediction)\"\n",
    "_, axes = plt.subplots(1, k, figsize=(15,3))\n",
    "for ax, image, label, pred in zip(axes, X_test[:k], t_test[:k], y):\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8,8)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Digit {}({})'.format(label, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Wdod7JEji96"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "Well... It is a bit awkward to introduce classification example for the regression module. Let us come back to the regression problem.\n",
    "\n",
    "This week, we will play with the [1985 Automobile Data Set](https://archive.ics.uci.edu/dataset/10/automobile) in [UCI Data Repository](https://archive.ics.uci.edu/ml/index.html).\n",
    "You can get the csv file directly from [data.csv](https://archive.ics.uci.edu/static/public/10/data.csv).\n",
    "You do not need to download the names file but you can read it to get informed about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYUfgRT0ji96"
   },
   "source": [
    "### Goal\n",
    "\n",
    "What we want to do with this data is developing a linear regression model that predicts the symboling, a variable used by actuarians to determine the risk of buying an automobile, given other input variables.\n",
    "\n",
    "Let us first import all the libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmRCtfogji96"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP_FzyhPji96"
   },
   "source": [
    "<div id='TODO1'></div>\n",
    "\n",
    "## Quick checking the content of the file\n",
    "\n",
    "\n",
    "Before start this TODO, make sure to download data file in the working directory.\n",
    "\n",
    "### TODO 1-1 (1 point)\n",
    "\n",
    "You can use `%pfile` (all) or `!head` (Linux or Mac) or `!type` (Windows) to quickly check the content of the file. See how the file is formatted so you can get an idea of how to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zeu0LieZji96",
    "outputId": "301be5de-0c46-45bd-e216-e6bebab18e6c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO 1-1: add pfile/head/type command here\n",
    "!powershell Get-Content -Head 30 imports-85.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATSErWeaji96"
   },
   "source": [
    "## Reading automobile data\n",
    "\n",
    "\n",
    "### TODO 1-2 (1 point)\n",
    "\n",
    "We are repeating what we did last week, loading data using pandas.\n",
    "\n",
    "1. Load the data file into variable `df` using pandas library.\n",
    "2. Print out the dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcWZu3Vaji96",
    "outputId": "0ef246d7-3e4d-430f-ac37-ecbd5895ffd9"
   },
   "outputs": [],
   "source": [
    "# TODO 1-2:\n",
    "DATA_URL = (\n",
    "    \"https://archive.ics.uci.edu/static/public/10/data.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(DATA_URL, nrows=1000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZI7CpDiji96"
   },
   "source": [
    "<div id='TODO1-2'></div>\n",
    "\n",
    "### TODO 1-3 (1 point)\n",
    "\n",
    "1. Let us look at the summary of data using `describe` again.\n",
    "2. Check the min, max, mean and standard deviation to get some idea of the value distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_ZkrbIbji97",
    "outputId": "902167f2-fe25-45c3-cbcb-85f8a64433ec"
   },
   "outputs": [],
   "source": [
    "# TODO 1-3:\n",
    "df_describe = df.describe()\n",
    "df_describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaFG_OS3ji97"
   },
   "source": [
    "### TODO 1-4 (2 points)\n",
    "1. Check for any null data in the dataset.\n",
    "2. Print the rows with null data.\n",
    "\n",
    "\n",
    "Hint: Refer to last week's lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkXMszK0ji97",
    "outputId": "fcdedc1a-5760-40ea-98c1-34589f77ce06"
   },
   "outputs": [],
   "source": [
    "# TODO 1-4.1: \n",
    "null_df = df.isnull()\n",
    "null_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NKP9LVVji97",
    "outputId": "a5487809-8635-4e83-fad5-d568051a7347"
   },
   "outputs": [],
   "source": [
    "# TODO 1-4.2:\n",
    "rows_with_null = df[df.isnull().any(axis=1)]\n",
    "rows_with_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oY9QbKhji97"
   },
   "source": [
    "<div id='TODO2'></div>\n",
    "\n",
    "### TODO 2 (5 points)\n",
    "\n",
    "1. Using `SimpleImputer` in Scikit-Learn, replace the missing values (NaN) with the most frequent values in the data. Store the cleaned data into `df_freq`.\n",
    "\n",
    "Hint: Refer to last week's lab exercise where you used the same to preprocess the garment workers productivity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiOEQlcUji97"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# TODO 2:\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_freq = pd.DataFrame(imputer.fit_transform(df))\n",
    "\n",
    "df_freq.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PyyHwQmji97"
   },
   "source": [
    "Notice that the names of each column has been changed to an integer, instead of a string. Let's get them back using the below code, and store the final database in the variable `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVP7lvdNji97",
    "outputId": "cb3aa8dd-902d-4e0d-e120-5d1d8be955b7"
   },
   "outputs": [],
   "source": [
    "df_freq.columns = df.columns\n",
    "df_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIyF3IZrji97",
    "outputId": "10d47315-6195-43b5-ebb5-4143e4bf370a"
   },
   "outputs": [],
   "source": [
    "df = df_freq\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI1rrnGwji97"
   },
   "source": [
    "Let's check the type of data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqqu974Iji97",
    "outputId": "8a4822dd-2dae-4780-a3da-96af147fad94"
   },
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXCyMTACji98"
   },
   "source": [
    "Notice that the type of data in each column is changed to object. Let's revert back them to numbers, which makes our work a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezF6L1Yuji98",
    "outputId": "abb4ec8c-1c85-4d5e-a363-59d99d0e44b5"
   },
   "outputs": [],
   "source": [
    "numeric_columns = ['normalized-losses', 'num-of-doors', 'wheel-base', 'length', 'width', 'height',\n",
    "                   'curb-weight', 'num-of-cylinders', 'engine-size', 'bore', 'stroke', 'compression-ratio',\n",
    "                   'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price', 'symboling']\n",
    "\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KUtfEiJji98"
   },
   "source": [
    "The list of column names in the variable `numeric_columns` are the ones which have numbers stored in them in the original data. The data in these column types is simply changed from type `object` to `int/float` by using the pandas function [to_numeric()](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html).\n",
    "\n",
    "But, what about the remaining data that is still present in the object form, to be specific, in the form of strings?\n",
    "\n",
    "Remember, in the last week's lab, we've converted each of them by manually assigning an integer to each string, and then modified the dataset using the pandas `apply()` and `lambda` functions. This task can be easily achieved by using `labelEncoder` function in the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZUW4BeIji98"
   },
   "source": [
    "<div id='TODO3'></div>\n",
    "\n",
    "### TODO 3 (5 points)\n",
    "\n",
    "1. Convert the data present in the form of strings in `df` to integer format using `labelEncoder` function. The list of columns with string data is given to you in the form of a list stored in `strings_list` variable.\n",
    "2. Hint: [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html),  [Reference](https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jg5uju2Lji98"
   },
   "outputs": [],
   "source": [
    "strings_list = ['make', 'fuel-type', 'aspiration', 'body-style',\n",
    "                'drive-wheels', 'engine-location', 'engine-type', 'fuel-system']\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# TODO 3:\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "for col in strings_list:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7bLY3V9ji98",
    "outputId": "efc3d689-478c-486c-e8d6-78f8c2cce723"
   },
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v8Ut_kiji98"
   },
   "source": [
    "Notice that our whole data has now been changed into numerical format, either in the form of int or float. This makes our work a lot easier when we go to advanced stages like training and testing a model. The preprocessed dataset is shown in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JXv5dDaji98",
    "outputId": "6ff44e01-40a0-4a01-9092-49773f17ac53"
   },
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im5X8Euoji9_"
   },
   "source": [
    "## Visualize the data\n",
    "\n",
    "Working for a linear regression, knowing the how the target value varies depending on input variables. So, by using scatter plot, let us see if there is positive or negative correlation between any input feature and the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0DONS9bji-A"
   },
   "source": [
    "<div id='TODO4'></div>\n",
    "\n",
    "### TODO 4-1 (3 points)\n",
    "\n",
    "1. In the last week's lab, we created 3 by 4 scatter plots. Similarly, create 5 by 5 scatter plots, each plotting the target value (symboling) against individual input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uETDzpedji-A",
    "outputId": "5c32d40a-de13-456f-cc02-d54dd96c2ead"
   },
   "outputs": [],
   "source": [
    "# TODO 4-1:\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "plt.clf() # Clear previous plt figure\n",
    "\n",
    "Target = df.loc[:, 'symboling']\n",
    "X_f = df.iloc[:, :-1]\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1) # Selects which subplot to plot to\n",
    "    plt.scatter(x = X_f.iloc[:, i], y=Target, marker= '.') # Plots a given column\n",
    "    plt.xlabel(X_f.columns.values[i]) # Sets x label\n",
    "    plt.ylabel(\"symboling\") # Sets Y label\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCxQtOMEji-A"
   },
   "source": [
    "Often, rather than just looking at how each feature interacts with the target, observing how each feature interacts with other features can be useful as well. One of the reasons for this is that we can begin to gain a glimpse at the dependency between features. If two highly correlated features exist, we can easily ignore one of them as using both would be redundant. Let say you have two input variable $𝑥1$ and $𝑥2$ and $𝑥2=2×𝑥1$. Then, your linear model $𝑦=𝑤1𝑥1+𝑤2𝑥2+𝑏$ can be easily converted to $𝑦=3𝑤𝑥1+𝑏$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRlvLpN5ji-A"
   },
   "source": [
    "### TODO 4-2 (2 points)\n",
    "\n",
    "1. Now, create a scatter plot which plots all the features against one another by using the Pandas `scatter_matrix()` function with our DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqOsovlrji-A",
    "outputId": "4685529c-62c8-4d39-dba4-bba0a50bfcc6"
   },
   "outputs": [],
   "source": [
    "# TODO 4-2:\n",
    "from pandas.plotting import scatter_matrix\n",
    "# Create a scatter matrix\n",
    "scatter_matrix(df, figsize=(10, 10), diagonal='kde', alpha=0.8, marker = \".\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nAdkzcKji-A"
   },
   "source": [
    "From above figures, we can see the target 'symboling' is a categorical value ranging from -2 to 3. Let us verify this using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G78abvHbji-A",
    "outputId": "d1d91394-79eb-4635-acac-23ad91557712"
   },
   "outputs": [],
   "source": [
    "df['symboling'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnytKj37ji-A"
   },
   "source": [
    "Yes, it is one of the six values, which  means we can use classification algorithms. We will talk about classification later but we can see how they are grouped together using Andrew's curve. Andrew's curve maps the high dimensional data into frequency plot using finite Fourier series. Therefore, you are expected to observe similar frequncy patterns in the graph for the similar data.  You can check David Andrew's paper ([Plots of High-Dimensional Data](https://www.jstor.org/stable/2528964), 1972)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEjGK6Dbji-A"
   },
   "source": [
    "### TODO 4-3 (3 points)\n",
    "\n",
    "1. Using Pandas `andrews_curve()` function in conjunction with our DataFrame `df` to produce the Andrew's curve. Take some time to observe the plot.\n",
    "    1. Try using different colours for each target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJDabjTgji-A",
    "outputId": "6ae54ab6-e9ce-4f6c-8cf7-d837c87a12fd"
   },
   "outputs": [],
   "source": [
    "# TODO 4-3:\n",
    "from pandas.plotting import andrews_curves\n",
    "# Andrews curve plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "andrews_curves(df, 'symboling', colormap='viridis')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmlXjE4Sji-B"
   },
   "source": [
    "What can you see from the plot?\n",
    "\n",
    "Well, there are large data samples overlap, so between -2 to 0, it is hard to see anything particular. Some samples in the symboling 1 and 3, however, sticks out a little, which will be interesting how this will impact on ML model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eU-IIoEji-B"
   },
   "source": [
    "Now, let us look at the target values.\n",
    "\n",
    "### TODO 4-4 (2 points)\n",
    "\n",
    "1. Create the histogram using our target represented by the 'symboling' column in our `df`. Take some time to observe the plot.\n",
    "    1. Hint: You can use Pandas or Matplotlib to  generate this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IA2aPmzIji-B",
    "outputId": "212cd2b4-67e1-480a-90d7-b8a4de829d58",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO 4-4:\n",
    "# Create the histogram\n",
    "df.loc[:, 'symboling'].plot.hist()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a52_L-Yuji-B"
   },
   "source": [
    "You can see the figure now shows the majority of automobiles are with symboling 0 and 1. Let us see how this sample imbalance will impact the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Fy-lerJji-B"
   },
   "source": [
    "## Splitting data into input features and targets\n",
    "\n",
    "Let us have two separate variables for input features and output targets.\n",
    "`X` will be used for the input features, `T` for target labels, and `N` for the total number of samples in our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LunplhQNji-B"
   },
   "source": [
    "<div id='TODO5'></div>\n",
    "\n",
    "### TODO 5-1 (3 points)\n",
    "\n",
    "1. Store the target column 'symboling' into `T`.\n",
    "2. Store all the input input features into `X`, excluding the target column `symboling`.\n",
    "3. Store the total number of samples in our dataset (e.g., rows) into `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5BdE_Sgji-B"
   },
   "outputs": [],
   "source": [
    "# TODO 5-1:\n",
    "\n",
    "T = df.loc[:, 'symboling'].copy() # copy the target into T\n",
    "X = df.iloc[:, :-1].copy()         # copy the feature into X\n",
    "N = df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qB7HHZ_Rji-B",
    "outputId": "27328c80-c943-4a53-9092-0beb79c2f7cd"
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    ('\"X\" in globals()', 'X is not defined'),\n",
    "    ('np.all(np.isclose(X.iloc[10, 1:5].values, np.array([2.0, 1.0, 0.0, 2.0])))', 'X has incorrect values'),\n",
    "    ('\"T\" in globals()', 'T is not defined'),\n",
    "    ('np.all(np.isclose(T.iloc[5:10].values, np.array([2, 1, 1, 1, 0])))', 'T has incorrect values'),\n",
    "    ('\"N\" in globals()', 'N is not defined'),\n",
    "    (\"N == 205\", \"N has the wrong value\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpaAZ4uBji-B"
   },
   "source": [
    "### TODO 5-2 (2 points)\n",
    "\n",
    "1. Split the data `X` and targets `T` using Sklearn's `train_test_split()` function. Store the values into `X_train`, `X_test`, `t_train`, and `t_test`. Be sure to pass the arguments that correspond to the following descriptions:\n",
    "    1. Split the data/targets using a 80/20 split (80% for training and 20% for testing).\n",
    "    1. Use a seed of 0 for the `random_state` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlkSEr7Uji-B",
    "outputId": "efc9e43e-416e-44d6-da68-108e09ac2b35"
   },
   "outputs": [],
   "source": [
    "# TODO 5-2:\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, T, test_size=0.2, random_state=0)\n",
    "print(X.shape, X_train.shape, X_test.shape, t_train.shape, t_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXIaaaizji-B",
    "outputId": "df1ef404-eaa3-45cf-f575-20b9ca755b6e"
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"X_train.shape == (164, 25)\", \"X_train has the wrong shape\"),\n",
    "    (\"X_test.shape == (41, 25)\", \"X_test has the wrong shape\"),\n",
    "    (\"t_train.shape == (164,)\", \"t_train has the wrong shape\"),\n",
    "    (\"t_test.shape == (41,)\", \"t_test has the wrong shape\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "604kVUUYji-C"
   },
   "source": [
    "# Applying Least Squares\n",
    "Now it's time to apply least squares to our data in an attempt to develop a model for predicting our target variable `symboling`! Recall that the least squares formula is used to generate weights $w$ which can then be used for making predictions given new data. The least squares formula is given below where the symbol $\\cdot$ corresponds to the dot product.\n",
    "\n",
    "$$\n",
    "w = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot T\n",
    "$$\n",
    "\n",
    "**References**\n",
    "\n",
    "If you want to gain a better understanding of what least squares is doing check out the following references.\n",
    "* [ Khan Academy](https://www.youtube.com/watch?v=MC7l96tW8V8)\n",
    "* [Geometric view](https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUpkeZLjji-C"
   },
   "source": [
    "<div id='TODO6'></div>\n",
    "\n",
    "### TODO 6 (5 points)\n",
    "\n",
    "Well, the model is implemented in `sklearn.linear_model`.\n",
    "\n",
    "1. Create an instance using the proper Sklearn class for conducting least squares. Store the output into `model`.\n",
    "    1. Hint: Make sure to import and use the right class. You can find the correct class to import and use by referring to the slides.\n",
    "2. Train the least squares model using the `X_train` and `t_train`.\n",
    "3. Evaluate the model by computing the test scores using the `score()` method with the `X_test` data and `t_test` targets. Store the output into `test_score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgAtCJhnji-C",
    "outputId": "4b0308bb-3e2f-4c35-a400-5a2fcc09c908"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# TODO 6:\n",
    "\n",
    "# 1) initialize\n",
    "# create a linear regression object\n",
    "model = LinearRegression()\n",
    "\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "\n",
    "# 3) evaluate\n",
    "\n",
    "test_score = model.score(X_test, t_test)\n",
    "\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnvPgqJaji-C",
    "outputId": "66d0df01-a970-42e3-e1a2-cbc3b6a879b9"
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    ('np.isclose(test_score,  0.387996, rtol=.1)', '`score` potentially has the wrong value.')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn_0uxICji-C"
   },
   "source": [
    "The `score` function calculated the coeffient $R^2$:\n",
    "$$\n",
    " R^2 = 1 - \\frac{(t - y)^2}{(t - \\bar{t})^2}\n",
    "$$\n",
    "where $t$ is target label, $y$ is predicted values, and $\\bar{t}$ is mean of target labels.\n",
    "When the model is perfect fit, $R^2 =1$. Knowing that, the model seems to be a bit weak. Let us check this with the following plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMYlEn4Dji-C"
   },
   "source": [
    "<div id='TODO7'></div>\n",
    "\n",
    "First, let us see how close the prediction is to the actual label by comparing the value against the diagonal line after plotting a scatter plot between them (See the figure below). If the predictions are accurate, all the points should lie on the diagonal line.\n",
    "\n",
    "### TODO 7-1 (3 points)\n",
    "1. call `predict()` method to make predictions for `X_test` input and store the results to `y`.  \n",
    "2. Plot `t_test` (x-axis) against `y` (y-axis).\n",
    "3. Based on your plot and results, briefly state what you think/observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uSzr1poji-C",
    "outputId": "e29f6d01-56c7-4398-99e8-3ccbad61563c"
   },
   "outputs": [],
   "source": [
    "# TODO 7-1:\n",
    "\n",
    "## TODO 1. make a prediction\n",
    "y = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(y.shape)\n",
    "print(\"First 5 predicted values:\", y[1:5])\n",
    "print(\"Expected values:\", np.array([-0.51629084, 1.938767, 1.35099362, 0.77757691]))\n",
    "print(\"Test score: \", test_score)\n",
    "\n",
    "## TODO 2. plot t_text vs y\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(t_test, y, alpha=0.7, marker = '.')\n",
    "\n",
    "# dashed diagonal line\n",
    "plt.plot([-3,3], [-3,3], 'r--')\n",
    "# x and y labels\n",
    "plt.xlabel(\"target\")\n",
    "plt.ylabel(\"predicted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNjlOvVVji-C",
    "outputId": "a27a14b5-4183-40ce-e102-4464b0159b78"
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"y.shape == (41,)\", \"y shape is incorrect\"),\n",
    "    (\"np.all(np.isclose(y[1:5], np.array([-0.51629084, 1.938767, 1.35099362, 0.77757691])))\", \"y values are incorrect\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLQ-txSNji-C"
   },
   "source": [
    "Here the red-diagonal line represents where the blue dots should fall if our classifier achieved 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCJvCZdyji-C"
   },
   "source": [
    "### TODO 7-2 (5 points)\n",
    "\n",
    "Write your thoughts about the results and plots in the below cell.\n",
    "\n",
    "**DO NOT WRITE YOUR ANSWER IN THIS CELL!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YepL6XJPji-C"
   },
   "source": [
    "`ANSWER: we could observe that the plot of predicted vs Actual Symbolism/target is along the diagonal. But the predicted shows great distances to the red diagonal. Some of the predicted are distant from the red line  `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi3q1Q5Hji-D"
   },
   "source": [
    "If you want to look at all the data samples plotted *with* their corresponding predictions, you can plot all the samples as in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyiteUltji-D",
    "outputId": "de789d06-9473-416e-fbdd-f08d4e50cc4c"
   },
   "outputs": [],
   "source": [
    "plt.plot(t_test.to_numpy(), '.')\n",
    "plt.plot(y, 'x')\n",
    "plt.xlabel(\"samples\")\n",
    "plt.ylabel(\"symboling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYppYV-Yji-D"
   },
   "source": [
    "Well, this above plot is a bit messy when we look at the results here. Let us plot each data sample in its own plot based on the  target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6XczmcMji-D"
   },
   "source": [
    "<div id='TODO8'></div>\n",
    "\n",
    "### TODO 8 (10 points)\n",
    "\n",
    "1. Create a sub-plot where each plot, plots the actual symboling target `t_test` (blue dot) against its predicted target `y` (orange x).\n",
    "  1. Hint: You will need to use sub-plots similar to TODO 4-1.\n",
    "  1. Hint: Notice, there are 5 unique values in `t_test`, thus you will need 5 sub-plots, one for each unique value in `t_test`. For example, let's say we have 2 data samples with the symboling '-1' in `t_test`. The plot for symboling '-1' should then plot the actual targets against the predicted targets (a total of 4 points should be plotted, 2 blue dots, 2 orange x's).\n",
    "2. Write your thoughts about what you observe from the figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSbqZQh5ji-D"
   },
   "source": [
    "Recall, `t_test` is a Pandas Series (1D DataFrame) and `y` is a numpy array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yse8DLhGji-D",
    "outputId": "c20f3d53-6846-481b-b01b-4a8bd4102b58"
   },
   "outputs": [],
   "source": [
    "t_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8yJqVHUji-D",
    "outputId": "196c4b51-b123-4d93-da68-ad72095493dc"
   },
   "outputs": [],
   "source": [
    "print(f\"t_test type {type(t_test)}\")\n",
    "print(f\"y type {type(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4eekG2Wji-D"
   },
   "source": [
    "Further, recall that the unique values inside `t_test` are as follows! Notice the symboling '-1' appears 2 times while the quality '2' appears 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc0dogd5ji-D",
    "outputId": "30795fc3-7810-4695-94b8-d882203095ee"
   },
   "outputs": [],
   "source": [
    "uniques, counts = np.unique(t_test, return_counts=True)\n",
    "\n",
    "print(f\"Unique values in t_test: {uniques}\")\n",
    "print(f\"Number of times each unqiue value appears in t_test: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG0SSukvji-D"
   },
   "source": [
    "Plotting the actual vs predicted for target '-1' might look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLZIGP0gji-D",
    "outputId": "ac56af9d-8672-45d3-cd9f-315bdce74f71"
   },
   "outputs": [],
   "source": [
    "plt.plot(t_test.values[t_test==-1], '.', label=\"Actual\")\n",
    "plt.plot(y[t_test==-1], 'x', label=\"Predicted\")\n",
    "plt.ylabel(\"Symboling\")\n",
    "plt.xlabel(\"Sample Number\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i63uHRpGji-E"
   },
   "source": [
    "Finish the code for TODO 8 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bh1cBB-Cji-E",
    "outputId": "ddaf125e-0293-4230-9474-5c256344a802"
   },
   "outputs": [],
   "source": [
    "# TODO 8:\n",
    "unique_values = sorted(dict.fromkeys(t_test.values)) \n",
    "fig , axes = plt.subplots(2, 3, figsize=(10,6)) \n",
    "axes = axes.ravel()  # Flatten the axes for easier iteration\n",
    "\n",
    "for i, val in enumerate(unique_values):\n",
    "    axes[i].plot(t_test.values[t_test==val], '.', label=\"Actual\")\n",
    "    axes[i].plot(y[t_test==val], 'x', label=\"Predicted\")\n",
    "    axes[i].set_ylabel(f\"Actual Symboling = {val}\")\n",
    "    axes[i].grid(alpha=0.3)\n",
    "# Hide any unused subplots\n",
    "for j in range(len(unique_values), len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0lb9bvLji-E"
   },
   "source": [
    "Write your thoughts about the results and plots in the below cell.\n",
    "\n",
    "**DO NOT WRITE YOUR ANSWER IN THIS CELL!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKkbBPFmji-E"
   },
   "source": [
    "`ANSWER: I see that the actual symboling is distant from the predicted. I can see that when t_test values are lower -1 to 0, the prediction is way above the actual and from 2 to 3 the prediction is way below. I can deduct that my model is not very accurate `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq_94ZyCji-E"
   },
   "source": [
    "\n",
    "As we'll be using other linear models throughout this notebook it will be nice to condense all the plots for examining our targets and predictions into one easy to call function. So, let's make a function that does this. We'll call it `evaluate`.\n",
    "\n",
    "<div id='TODO9'></div>\n",
    "\n",
    "### TODO 9 (5 points)\n",
    "\n",
    "Finish coding the `evaluate()`function given below.\n",
    "\n",
    "1. Plot `t_test` (x-axis) against `y` (y-axis) by calling the function we finished in TODO 7.1.\n",
    "2. Plot all the data samples `t_test` *with* their corresponding predictions `y` using the function we defined for you a few code cells earlier.\n",
    "3.  Create a sub-plot where each plot, plots the actual target `t_test` (blue dot) against its predicted target `y` (orange x) as we did in TODO 8.\n",
    "4. Maintaining the subplot structure as it is, fill in the blank to finish the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABMErKFGji-E"
   },
   "outputs": [],
   "source": [
    "def evaluate(y, t):\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    # TODO 9:\n",
    "    # t vs y plot\n",
    "    plt.subplot(3,3, 1)\n",
    "    # TODO: add the first plot\n",
    "    plt.scatter(t_test, y, alpha=0.7, marker = '.')\n",
    "    # dashed diagonal line\n",
    "    plt.plot([-3,3], [-3,3], 'r--')\n",
    "    plt.xlabel(\"target\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Plot-1\")\n",
    "\n",
    "    # all value comparison\n",
    "    plt.subplot(3,2, 2)\n",
    "    # TODO: add the second one\n",
    "    plt.plot(t_test.to_numpy(), '.')\n",
    "    plt.plot(y, 'x')\n",
    "    plt.xlabel(\"samples\")\n",
    "    plt.ylabel(\"symboling\")\n",
    "    plt.title(\"Plot-2\")\n",
    "\n",
    "\n",
    "    # subplots of individual quality comparision\n",
    "    # TODO: add the third subplots\n",
    "    unique_values = sorted(dict.fromkeys(t_test.values)) \n",
    "    fig , axes = plt.subplots(2, 3, figsize=(10,6)) \n",
    "    axes = axes.ravel()  # Flatten the axes for easier iteration\n",
    "    \n",
    "    for i, val in enumerate(unique_values):\n",
    "        axes[i].plot(t_test.values[t_test==val], '.', label=\"Actual\")\n",
    "        axes[i].plot(y[t_test==val], 'x', label=\"Predicted\")\n",
    "        axes[i].set_ylabel(f\"Actual Symboling = {val}\")\n",
    "        axes[i].grid(alpha=0.3)\n",
    "    # Hide any unused subplots\n",
    "    for j in range(len(unique_values), len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-ivEeaIji-E"
   },
   "source": [
    "When finishing the function, you can call the function as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKgCMQ-4ji-E",
    "outputId": "4246e340-2d9b-4a2f-ffaa-45a497e80782"
   },
   "outputs": [],
   "source": [
    "evaluate(y, t_test.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2qm51y5ji-E"
   },
   "source": [
    "Can you take a guess to what these values we are accessing from our `model` correspond to? these values are the weights and the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdih8L7Hji-E",
    "outputId": "c0dec8bc-63b0-435a-e714-8e31d6f7e22c"
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1KGojGcji-E",
    "outputId": "8c076c39-d24a-4306-8390-8ef69684aa43"
   },
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7QnRpxHji-E"
   },
   "source": [
    "## Weight Observation\n",
    "\n",
    "The previous cells print out some parameters from our model, specifically the weights and y-intercept (bias value) for our linear model. That's cool, but what these parameters tell us?\n",
    "\n",
    "Often, the weights contain meaningful information to understand the data and machine learning model. For instance, as we'll see in the following figure as well, the weights can inform us about how much each feature/variable contributes to the models predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDxE0EDfji-E"
   },
   "source": [
    "<div id='TODO10'></div>\n",
    "\n",
    "### TODO 10 (5 points)\n",
    "Let's make a bar plot that plots the values of our weights and bias. To make presentation as informative as possible, we'll also add the values on top of the bar chart.\n",
    "\n",
    "1. Read the code below and find the TODO comment. Add one line of code to create a bar plot using matplotlib `plt.bar()` function to plot the values of the weights/bias stored in `w`. Store the output into `rects`.\n",
    "\n",
    "2. Call the `autolabel()` function passing the correct arguments to plot the values of the weights/bias above the bars in the bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSmyCCEpji-F"
   },
   "outputs": [],
   "source": [
    "\n",
    "# print the value text over the bar\n",
    "# https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html\n",
    "def autolabel(ax, rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{:0.3f}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', color='blue')\n",
    "\n",
    "def show_weights(model, names):\n",
    "\n",
    "    # combine both the coefficients and intercept to present\n",
    "    w = np.append(model.coef_, model.intercept_)\n",
    "\n",
    "    names = list(names) + ['bias/intercept']\n",
    "\n",
    "    plt.figure(figsize=(12,3))\n",
    "\n",
    "    # TODO: create bar chart to present the weights\n",
    "    rects = plt.bar(range(len(w)), w, color='skyblue')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(range(len(w)))\n",
    "    ax.set_xticklabels(names, rotation = 90)\n",
    "\n",
    "    # TODO: call the autolabel function\n",
    "    autolabel(ax, rects)\n",
    "    plt.title(\"Model Weights and Bias\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "executionInfo": {
     "elapsed": 603,
     "status": "error",
     "timestamp": 1725751243871,
     "user": {
      "displayName": "Xiang Zhang",
      "userId": "09382339014389145451"
     },
     "user_tz": 240
    },
    "id": "f_2At-O8x69c",
    "outputId": "37bbc072-dc0c-468c-9436-0d2b776d51fd"
   },
   "outputs": [],
   "source": [
    "show_weights(model, df.columns.values[:-1]) # We dropped the last column in TODO 5, so here we drop the name of the last column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELgDFa8-ji-F"
   },
   "source": [
    "It looks `fuel-type` must have highest conntribution with the large bias. But here let us hold this interpretation. Remember that the scale of input vairables are not similar, which means the current interpretation must be **misleading**.\n",
    "\n",
    "For now, let us observe/analyze without considering input scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxAy8sciji-F"
   },
   "source": [
    "# Regularized Linear Models\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Now let's repeat everything we just did but now let's look at using linear regression with regularization. To start off, let's try using ridge regression and see if the result differ at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM-eL3kkji-F"
   },
   "source": [
    "<div id='TODO11'></div>\n",
    "\n",
    "### TODO 11-1 (5 points)\n",
    "\n",
    "1. Create an instance using the proper Sklearn class for conducting ridge regression. Store the output into `model`.\n",
    "    1. Hint: Make sure to import and use the right class. You can find the correct class to import and use by referring to the slides.\n",
    "2. Train the ridge regression model using the `X_train` and `t_train`.\n",
    "3. Evaluate the model by computing the test scores using the `score()` method with the `X_test` data and `t_test` targets. Store this value into `test_score`.\n",
    "4. Using `model`, make a prediction using `predict()` method taking `X_test` as input. Store the output inside `y`\n",
    "5. Call the `evaluate()` function and observe the results.\n",
    "6. Call the `show_weights()` function and analyze the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViZa4jAeji-F",
    "outputId": "e6a92551-70da-4444-aeea-bdcce03bc5ce"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# 1) initialize\n",
    "model = Ridge(alpha=0.5) \n",
    "\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "\n",
    "# 3) evaluate\n",
    "test_score = model.score(X_test, t_test)\n",
    "\n",
    "print(\"Test score: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cqo8nkOEji-F",
    "outputId": "f2bbe148-c059-49cd-82f1-1644c0885c12"
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    ('test_score > 0.30', '`test_score` is below .30, try adusting the `alpha` value.')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_8sflmWji-F"
   },
   "outputs": [],
   "source": [
    "# 4) Predict the values\n",
    "y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx1tFBKTji-F",
    "outputId": "e2842b99-4303-4062-8f08-dc40dcba50e8"
   },
   "outputs": [],
   "source": [
    "# 5) Plot actual vs predicted graphs\n",
    "evaluate(y, t_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLeVwbRqji-F",
    "outputId": "89b8895b-6400-45ce-bb22-2469ad306794"
   },
   "outputs": [],
   "source": [
    "# 6) Plot the weights\n",
    "show_weights(model, df.columns.values[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOopJTL5ji-F"
   },
   "source": [
    "## Lasso Regression\n",
    "\n",
    "Now let's try lasso regression!\n",
    "\n",
    "\n",
    "### TODO 11-2 (5 points)\n",
    "\n",
    "1. Combine all the process and the modify the code to use Lasso. You can simply copy the four above code cells and modify it to repeat the same process but with Lasso this time.\n",
    "\n",
    "    1. Create an instance using the proper Sklearn class for conducting lasso regression. Store the output into `model`.\n",
    "        1. Hint: Make sure to import and use the right class. You can find the correct class to import and use by referring to the slides.\n",
    "    2. Train the lasso regression model using the `X_train` and `t_train`.\n",
    "    3. Evaluate the model by computing the test scores using the `score()` method with the `X_test` data and `t_test` targets. Store this value into `test_score`.\n",
    "    4. Using `model`, make a prediction using `predict()` method taking `X_test` as input. Store the output inside `y`\n",
    "    5. Call the `evaluate()` function and observe the results.\n",
    "    6. Call the `show_weights()` function and analyze the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqsJEHF3ji-F",
    "outputId": "94a800fc-5c0a-420e-9500-645bc624cb9f"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 1) initialize\n",
    "model = Lasso(alpha=0.1)\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "# 3) evaluate\n",
    "test_score = model.score(X_test, t_test)\n",
    "print(\"Test score: \", test_score)\n",
    "\n",
    "# 4) Predict the values\n",
    "y = model.predict(X_test)\n",
    "# 5) Plot actual vs predicted graphs\n",
    "evaluate(y, t_test.to_numpy())\n",
    "# 6) Plot the weights\n",
    "show_weights(model, df.columns.values[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-AyhQnFji-G",
    "outputId": "911f0f1d-bd58-4d6d-beff-395566af0e5c"
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    ('test_score > 0.40', '`test_score` is below .40, try adusting the `alpha` value.')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIBe7WBcji-G"
   },
   "source": [
    "## Elastic Net\n",
    "\n",
    "### TODO 11-3 (5 points)\n",
    "\n",
    "1. Combine all the process and the modify the code to use Elastic Net. You can simply copy the above cell and modify it to repeat the same process but with Elastic Net this time.\n",
    "\n",
    "    1. Create an instance using the proper Sklearn class for elastic net. Store the output into `model`.\n",
    "        1. Hint: Make sure to import and use the right class. You can find the correct class to import and use by referring to the slides.\n",
    "    2. Train the elastic net model using the `X_train` and `t_train`.\n",
    "    3. Evaluate the model by computing the test scores using the `score()` method with the `X_test` data and `t_test` targets. Store this value into `test_score`.\n",
    "    4. Using `model`, make a prediction using `predict()` method taking `X_test` as input. Store the output inside `y`\n",
    "    5. Call the `evaluate()` function and observe the results.\n",
    "    6. Call the `show_weights()` function and analyze the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "478Gf54Wji-G",
    "outputId": "19f4d241-99a7-429d-e1eb-3ca48aa8c73d"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# 1) initialize\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "# 3) evaluate\n",
    "test_score = model.score(X_test, t_test)\n",
    "print(\"Test score: \", test_score)\n",
    "\n",
    "# 4) Predict the values\n",
    "y = model.predict(X_test)\n",
    "# 5) Plot actual vs predicted graphs\n",
    "evaluate(y, t_test.to_numpy())\n",
    "# 6) Plot the weights\n",
    "show_weights(model, df.columns.values[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKN68cuQji-G",
    "outputId": "a9685990-c0cf-42e1-bc21-5da16584d263"
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    ('test_score > 0.40', '`test_score` is below .40, try adusting the `alpha` value.')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jha6Tq1Pji-G"
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "### TODO 11-4 (5 points)\n",
    "\n",
    "1. Combine all the process and the modify the code to use SGD. You can simply copy the above cell and modify it to repeat the same process but with SGD this time.\n",
    "\n",
    "    1. Create an instance using the proper Sklearn class for conducting SGD. Store the output into `model`.\n",
    "        1. Hint: Make sure to import and use the right class. You can find the correct class to import and use by referring to the slides.\n",
    "    2. Train the SGD model using the `X_train` and `t_train`.\n",
    "    3. Evaluate the model by computing the test scores using the `score()` method with the `X_test` data and `t_test` targets. Store this value into `test_score`.\n",
    "    4. Using `model`, make a prediction using `predict()` method taking `X_test` as input. Store the output inside `y`\n",
    "    5. Call the `evaluate()` function and observe the results.\n",
    "    6. Call the `show_weights()` function and analyze the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oADwyKGzji-G",
    "outputId": "4f661cf6-bbdc-4b4b-8582-a67523577ab0"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# 1) initialize\n",
    "model = SGDRegressor(max_iter=1000, tol=1e-3, penalty='l2', alpha=0.03, random_state=42)\n",
    "\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "# 3) evaluate\n",
    "test_score = model.score(X_test, t_test)\n",
    "print(\"Test score: \", test_score)\n",
    "\n",
    "# 4) Predict the values\n",
    "y = model.predict(X_test)\n",
    "# 5) Plot actual vs predicted graphs\n",
    "evaluate(y, t_test.to_numpy())\n",
    "# 6) Plot the weights\n",
    "show_weights(model, df.columns.values[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    ('test_score < 0', '`test_score` should be an extreme negative vaue')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7uo7DGHji-G"
   },
   "source": [
    "<div id=\"TODO12\"></div>\n",
    "\n",
    "### TODO 12 (5 points)\n",
    "\n",
    "Write your thoughts about the plots above. Pay attention to the values of the weights, the predictions, and the test score we got.\n",
    "\n",
    "**DO NOT WRITE YOUR ANSWER IN THIS CELL!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CNh1Pz8ji-G"
   },
   "source": [
    "`ANSWER: General Observations\n",
    "For all the models (linear Regression, Ridge Regression, Lasso Regression, Elastic Net) we could observe that the plot of predicted vs Actual Symbolism/target is along the diagonal.\n",
    "-\tThe Test Scores are poor all below 40%\n",
    "For, and Stochastic Gradient Descent the red line is horizontal\n",
    "-\tThe Test Score is negative\n",
    "Test Scores & Model Comparison\n",
    "Linear Regression: test score = 0.3879966145620023\n",
    "Ridge: test score = 0.3888197852975577\n",
    "Lasso Regression: test score = 0.4277618447604027\n",
    "Elastic Net: test score = 0.45726770065887723\n",
    "Stochastic Gradient Descent: test score = -6.261884166999523e+34\n",
    "Linear Regression and Ridge seem to be performing similarly. Lasso and Elastic Net have close test scores.\n",
    "SGD has a negative test score.\n",
    "Model Weight and Bias: in the Linear regression fuel-type seems to be significant among all the features.\n",
    "The bias/intercept: the bias/intercept seems to be higher for the Lasso Regression.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_EFw4huji-G"
   },
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unbfShx3ji-G"
   },
   "source": [
    "<div id='TODO13'> </div>\n",
    "\n",
    "### TODO 13 (10 points)\n",
    "\n",
    "Let us streamlit the models for comparison by using `evaluate` and `show_weights`.\n",
    "Your final interface does not need to be exactly same but to secure an credit, you need to\n",
    "1. Include all the models we practiced\n",
    "2. The parameters are controllable so you can observe the effects of choosing different values interactively.\n",
    "3. You should use both `evaluate` and `show_weights` at minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bzR4Ji6ji-H",
    "outputId": "814a627a-1a40-457c-d2ef-34a56c1cd043"
   },
   "outputs": [],
   "source": [
    "%%writefile automobile_linearReg.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "# TODO: Import all the linear regression models that you used here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "# SETTING PAGE CONFIG TO WIDE MODE\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "# LOADING DATA\n",
    "DATA_URL = (\n",
    "    \"https://archive.ics.uci.edu/static/public/10/data.csv\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "# 1985 Auto Imports Database\n",
    "\n",
    "Abstract: This data set consists of three types of entities:\n",
    "        (a) the specification of an auto in terms of various characteristics,\n",
    "        (b) its assigned insurance risk rating,\n",
    "        (c) its normalized losses in use as compared to other cars. The second rating\n",
    "        corresponds to the degree to which the auto is more risky than its price indicates.\n",
    "        Cars are initially assigned a risk factor symbol associated with its\n",
    "        price.   Then, if it is more risky (or less), this symbol is\n",
    "        adjusted by moving it up (or down) the scale.  Actuarians call this\n",
    "        process \"symboling\".  A value of +3 indicates that the auto is\n",
    "        risky, -3 that it is probably pretty safe.\n",
    "\"\"\"\n",
    "\n",
    "@st.cache_data\n",
    "def load_data(nrows):\n",
    "    # TODO: Import the data, preprocess it, and seperate it into training and testing data\n",
    "    df = pd.read_csv(DATA_URL, nrows=nrows)\n",
    "    #is there missing values\n",
    "    null_df = df.isnull()\n",
    "    null_df\n",
    "\n",
    "    rows_with_null = df[df.isnull().any(axis=1)]\n",
    "    rows_with_null\n",
    "    #Using SimpleImputer in Scikit-Learn, replace the missing values (NaN) with the most \n",
    "    #frequent values in the data. Store the cleaned data into df_freq.\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df_freq = pd.DataFrame(imputer.fit_transform(df))\n",
    "    df_freq\n",
    "    df_freq.head()\n",
    "    df_freq.columns = df.columns\n",
    "    df_freq.head()\n",
    "\n",
    "    df = df_freq\n",
    "    \"Dataframe\", df.head()\n",
    "    \n",
    "    #Let's check the type of data in each column\n",
    "    df.dtypes\n",
    "    numeric_columns = ['normalized-losses', 'num-of-doors', 'wheel-base', 'length', 'width', 'height',\n",
    "                    'curb-weight', 'num-of-cylinders', 'engine-size', 'bore', 'stroke', 'compression-ratio',\n",
    "                    'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price', 'symboling']\n",
    "\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    df.dtypes\n",
    "    #Convert the data present in the form of strings in df to integer format using labelEncoder function. \n",
    "    #The list of columns with string data is given to you in the form of a list stored in strings_list\n",
    "    strings_list = ['make', 'fuel-type', 'aspiration', 'body-style',\n",
    "                'drive-wheels', 'engine-location', 'engine-type', 'fuel-system']\n",
    "\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    for col in strings_list:\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "    df.dtypes\n",
    "    # let's lookk at df\n",
    "    \"DataFrame df now: \", df\n",
    "\n",
    "\n",
    "    T = df.loc[:, 'symboling'].copy() # copy the target into T\n",
    "    X = df.iloc[:, :-1].copy()         # copy the feature into X\n",
    "    N = df.shape[0]\n",
    "    \"X.shape\", X.shape\n",
    "\n",
    "    # Split the data into training and testing sets (80/20 split)\n",
    "    X_train, X_test, t_train, t_test = train_test_split(X, T, test_size=0.2, random_state=0)\n",
    "    X.shape, X_train.shape, X_test.shape, t_train.shape, t_test.shape\n",
    "\n",
    "    return df, X_train, X_test, t_train, t_test\n",
    "    \n",
    "df, X_train, X_test, t_train, t_test = load_data(100000)\n",
    "\n",
    "\n",
    "\"## Summary\"\n",
    "st.dataframe(df.describe())\n",
    "\n",
    "\n",
    "#################### functions\n",
    "\n",
    "def evaluate(y, t):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    # Paste the corresponding part of your evaluate() function\n",
    "    # t vs y plot\n",
    "    plt.subplot(3,3, 1)\n",
    "    # TODO: add the first plot\n",
    "    plt.scatter(t_test, y, alpha=0.7, marker = '.')\n",
    "    # dashed diagonal line\n",
    "    plt.plot([-3,3], [-3,3], 'r--')\n",
    "    plt.xlabel(\"target\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Plot-1\")\n",
    "\n",
    "    # all value comparison\n",
    "    plt.subplot(3,2, 2)\n",
    "    # TODO: add the second one\n",
    "    plt.plot(t_test.to_numpy(), '.')\n",
    "    plt.plot(y, 'x')\n",
    "    plt.xlabel(\"samples\")\n",
    "    plt.ylabel(\"symboling\")\n",
    "    plt.title(\"Plot-2\")\n",
    "    st.pyplot(fig)  \n",
    "\n",
    "    # subplots of individual quality comparision\n",
    "    # TODO: add the third subplots\n",
    "    unique_values = sorted(dict.fromkeys(t_test.values)) \n",
    "    fig , axes = plt.subplots(2, 3, figsize=(10,6)) \n",
    "    axes = axes.ravel()  # Flatten the axes for easier iteration\n",
    "\n",
    "    for i, val in enumerate(unique_values):\n",
    "        axes[i].plot(t_test.values[t_test==val], '.', label=\"Actual\")\n",
    "        axes[i].plot(y[t_test==val], 'x', label=\"Predicted\")\n",
    "        axes[i].set_ylabel(f\"Actual Symboling = {val}\")\n",
    "        axes[i].grid(alpha=0.3)\n",
    "    # Hide any unused subplots\n",
    "    for j in range(len(unique_values), len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "    st.pyplot(fig)\n",
    "\n",
    "# print the value text over the bar\n",
    "# https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html\n",
    "def autolabel(ax, rects):\n",
    "    # Paste the corresponding part of your autolabel() function\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{:0.3f}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', color='blue')\n",
    "            \n",
    "\n",
    "def show_weights(model, names):\n",
    "\n",
    "    # combine both the coefficients and intercept to present\n",
    "    w = np.append(model.coef_, model.intercept_)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,3))\n",
    "\n",
    "    # Paste the corresponding part of your show_weights() function\n",
    "    names = list(names) + ['bias/intercept']\n",
    "\n",
    "    # create bar chart to present the weights\n",
    "    rects = plt.bar(range(len(w)), w, color='skyblue')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(range(len(w)))\n",
    "    ax.set_xticklabels(names, rotation = 90)\n",
    "\n",
    "    # TODO: call the autolabel function\n",
    "    autolabel(ax, rects)\n",
    "    plt.title(\"Model Weights and Bias\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    #plt.tight_layout()\n",
    "\n",
    "    st.pyplot(fig)\n",
    "####################\n",
    "\n",
    "\n",
    "st.divider()\n",
    "# TODO: Add your code to observe different models\n",
    "'''\n",
    "# Linear Regression\n",
    "'''\n",
    "np.random.seed(0)\n",
    "\n",
    "# 1) initialize\n",
    "# create a linear regression object\n",
    "model = LinearRegression()\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "# 3) evaluate\n",
    "test_score = model.score(X_test, t_test)\n",
    "test_score\n",
    "# 4) Predict the values\n",
    "y = model.predict(X_test)\n",
    "# 5) Plot actual vs predicted graphs\n",
    "evaluate(y, t_test.to_numpy())\n",
    "# 6) Plot the weights\n",
    "show_weights(model, df.columns.values[:-1])\n",
    "\n",
    "st.divider()\n",
    "\n",
    "'''\n",
    "# Ridge Regression\n",
    "Now let's repeat everything we just did but now let's look at using linear regression with regularization. \n",
    "To start off, let's try using ridge regression and see if the result differ at all!\n",
    "'''\n",
    "# 1) initialize\n",
    "model = Ridge(alpha=1.0) \n",
    "\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "\n",
    "# 3) evaluate\n",
    "test_score = model.score(X_test, t_test)\n",
    "\"Test score: \", test_score\n",
    "\n",
    "# 4) Predict the values\n",
    "y = model.predict(X_test)\n",
    "# 5) Plot actual vs predicted graphs\n",
    "evaluate(y, t_test.to_numpy())\n",
    "# 6) Plot the weights\n",
    "show_weights(model, df.columns.values[:-1])\n",
    "\n",
    "st.divider()\n",
    "\n",
    "'''\n",
    "# Lasso Regression\n",
    "'''\n",
    "# 1) initialize\n",
    "model = Lasso(alpha=0.1)\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "# 3) evaluate\n",
    "test_score = model.score(X_test, t_test)\n",
    "\"Test score: \", test_score\n",
    "\n",
    "# 4) Predict the values\n",
    "y = model.predict(X_test)\n",
    "# 5) Plot actual vs predicted graphs\n",
    "evaluate(y, t_test.to_numpy())\n",
    "# 6) Plot the weights\n",
    "show_weights(model, df.columns.values[:-1])\n",
    "\n",
    "st.divider()\n",
    "\n",
    "'''\n",
    "# Elastic Net\n",
    "'''\n",
    "# 1) initialize\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "# 3) evaluate\n",
    "test_score = model.score(X_test, t_test)\n",
    "\"Test score: \", test_score\n",
    "\n",
    "# 4) Predict the values\n",
    "y = model.predict(X_test)\n",
    "# 5) Plot actual vs predicted graphs\n",
    "evaluate(y, t_test.to_numpy())\n",
    "# 6) Plot the weights\n",
    "show_weights(model, df.columns.values[:-1])\n",
    "st.divider()\n",
    "\n",
    "'''\n",
    "# Stochastic Gradient Descent\n",
    "'''\n",
    "# 1) initialize\n",
    "model = SGDRegressor(max_iter=1000, tol=1e-3, penalty='l2', alpha=0.03, random_state=42)\n",
    "\n",
    "# 2) train the model\n",
    "model.fit(X_train, t_train)\n",
    "# 3) evaluate\n",
    "test_score = model.score(X_test, t_test)\n",
    "\"Test score: \", test_score\n",
    "\n",
    "# 4) Predict the values\n",
    "y = model.predict(X_test)\n",
    "# 5) Plot actual vs predicted graphs\n",
    "evaluate(y, t_test.to_numpy())\n",
    "# 6) Plot the weights\n",
    "show_weights(model, df.columns.values[:-1])\n",
    "\n",
    "st.divider()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmecKJAbji-H"
   },
   "source": [
    "### Expected Output:\n",
    "\n",
    "\n",
    "![image.png](https://webpages.charlotte.edu/mlee173/teach/ml/images/class/lab3-streamlit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8o4YFmtji-H"
   },
   "source": [
    "<div id=\"feedback\"></div>\n",
    "\n",
    "## Feedback (2 points)\n",
    "\n",
    "Did you enjoy the lab?\n",
    "\n",
    "Please take time to answer the following feedback qustions to help us further improve these labs! Your feedback is crucial to making these labs more useful!\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9v1n8Iqji-H"
   },
   "source": [
    "* How do you rate the overall experience in this lab? (5 likert scale. i.e., 1 - poor ... 5 - amazing)  \n",
    "Why do you think so? What was most/least useful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdfB2U1gji-H"
   },
   "source": [
    "`ANSWER: on a Likert scale of 1 to 5, I think this lab is a 5 - amazing because it instructs step by step. it built on the previous lab on processing the data, then it takes each linear regression model and shows how they are used in a practical way using sklearn.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYsw_u1qji-H"
   },
   "source": [
    "* What did you find difficult about the lab? Were there any TODOs that were unclear? If so, what specfically did not make sense about it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC1aAPVnji-H"
   },
   "source": [
    "`ANSWER: the loading of the data was misleading; I first downloaded the data by clicking the link that was given, and I realized that the format of the data that I had was not the same as the results presented in the lab. Until I got to the stream where I saw I could open it from id URL.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzUJB-mTji-H"
   },
   "source": [
    "* Which concepts, if any, within the lab do you feel could use more explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3VeSGWsji-H"
   },
   "source": [
    "`ANSWER: The Stochastic Gradient Descent was a little unclear to me. I also have issues interpreting the plottings.`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
